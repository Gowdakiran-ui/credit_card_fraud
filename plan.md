# Real-Time Credit Card Fraud & Risk Prediction Pipeline
## Production-Grade ML Systems Architecture Specification

**Version:** 1.0  
**Author:** Gowda Kiran
**Target Audience:** Senior Engineers, Architects, Interviewers  
**Project Type:** Portfolio / Production-Ready System Design

---

## Table of Contents
1. [Executive Summary](#executive-summary)
2. [Context & Legacy System Analysis](#context--legacy-system-analysis)
3. [Phase 1: Problem Scoping & Metrics](#phase-1-problem-scoping--metrics)
4. [System Architecture Overview](#system-architecture-overview)
5. [Sprint Planning: 4-Week Implementation](#sprint-planning-4-week-implementation)
6. [Senior Review Edge Cases](#senior-review-edge-cases)
7. [Technical Debt & Roadmap](#technical-debt--roadmap)
8. [Appendix: References & Resources](#appendix-references--resources)

---

## Executive Summary

This document outlines the design and implementation of a **real-time credit card fraud detection system** capable of processing transactions with <200ms latency while maintaining high precision and recall. The system replaces a legacy rule-based engine with a modern ML pipeline featuring:

- **Streaming Architecture**: Kafka-based event processing
- **Real-Time Feature Engineering**: Redis-backed feature store for velocity features
- **Low-Latency Inference**: FastAPI + containerized model serving
- **Production Monitoring**: Prometheus metrics, data drift detection, model performance tracking

**Key Differentiators:**
- End-to-end latency budget management
- Fail-safe strategies (fail-open vs fail-closed)
- Shadow deployment capability for safe model rollouts
- Comprehensive observability and alerting

---

## Context & Legacy System Analysis

### The Legacy System: Rule-Based Fraud Detection


``

**Pain Points:**

| Issue | Impact | Business Cost |
|-------|--------|---------------|
| **High False Positive Rate (8-12%)** | Legitimate customers blocked, poor UX | ~$2.4M annually in lost transactions |
| **Static Rules** | Cannot adapt to evolving fraud patterns | Fraud losses increased 23% YoY |
| **Slow Response Time** | 350-500ms average latency | Cannot support real-time authorization flows |
| **Manual Maintenance** | Rules updated quarterly by analysts | 40 hours/month analyst time |
| **No Contextual Learning** | Treats all users identically | Misses sophisticated fraud patterns |

**Why It's Failing:**

1. **Adversarial Adaptation**: Fraudsters reverse-engineer rules within weeks
2. **Threshold Brittleness**: Single-value thresholds (e.g., `amount > 5000`) miss context
3. **No Temporal Patterns**: Cannot detect subtle velocity changes or behavioral shifts
4. **Cold Start Problem**: New users/merchants have no historical context
5. **Operational Overhead**: Rule conflicts and maintenance burden



---

## Phase 1: Problem Scoping & Metrics

### Business Objective

**Primary Goal**: Minimize customer friction (false positives) while maximizing fraud detection (recall)

**Stakeholder Requirements:**
- **Risk Team**: Detect ‚â•95% of fraudulent transactions (Recall ‚â• 0.95)
- **Product Team**: Keep false positive rate <3% (Precision ‚â• 0.97)
- **Engineering**: End-to-end latency <200ms (p99)
- **Compliance**: Explainability for flagged transactions (GDPR/FCRA)

### Metrics Framework

#### Primary Metric: **Precision-Recall AUC (PR-AUC)**

**Why PR-AUC over ROC-AUC?**
- Fraud is a **highly imbalanced problem** (~0.1-0.5% fraud rate)
- ROC-AUC is overly optimistic with class imbalance
- PR-AUC focuses on positive class (fraud) performance

**Target**: PR-AUC ‚â• 0.85 (vs. legacy system's ~0.62)

#### Secondary Metrics

| Metric | Target | Measurement Window | Alert Threshold |
|--------|--------|-------------------|-----------------|
| **Precision @ 95% Recall** | ‚â•0.90 | Daily | <0.85 |
| **False Positive Rate** | <3% | Hourly | >5% |
| **Inference Latency (p99)** | <150ms | Real-time | >200ms |
| **Model Staleness** | <7 days | Continuous | >10 days |
| **Data Drift (PSI)** | <0.2 | Daily | >0.25 |

#### Business Metrics

- **Customer Friction Rate**: % of legitimate users experiencing declines
- **Fraud Loss Prevention**: $ amount of fraud caught
- **Operational Efficiency**: Hours saved vs. manual review

### Latency Constraint: <200ms End-to-End

**Latency Budget Breakdown:**

```
Total Budget: 200ms
‚îú‚îÄ‚îÄ Network Overhead (API Gateway ‚Üí Service): 20ms
‚îú‚îÄ‚îÄ Feature Retrieval (Redis): 30ms
‚îú‚îÄ‚îÄ Feature Engineering: 25ms
‚îú‚îÄ‚îÄ Model Inference: 80ms
‚îú‚îÄ‚îÄ Post-Processing & Business Logic: 25ms
‚îî‚îÄ‚îÄ Response Serialization: 20ms
```

**Critical Path Optimization:**
- Redis feature store with <10ms p99 latency
- Model quantization (FP32 ‚Üí INT8) for 2-3x speedup
- Async feature fetching where possible
- Connection pooling for all external services

---

## System Architecture Overview

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transaction    ‚îÇ
‚îÇ  Source (POS,   ‚îÇ
‚îÇ  Online, ATM)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Kafka Topic: transactions               ‚îÇ
‚îÇ  (Partitioned by card_id, 3 replicas, retention=7d) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                  ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Feature    ‚îÇ   ‚îÇ   Fraud      ‚îÇ  ‚îÇ   Archive    ‚îÇ
‚îÇ  Engineering ‚îÇ   ‚îÇ  Detection   ‚îÇ  ‚îÇ   Consumer   ‚îÇ
‚îÇ   Consumer   ‚îÇ   ‚îÇ   Service    ‚îÇ  ‚îÇ  (S3/Data    ‚îÇ
‚îÇ              ‚îÇ   ‚îÇ   (FastAPI)  ‚îÇ  ‚îÇ   Lake)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                  ‚îÇ
       ‚ñº                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ    Redis     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ Feature Store‚îÇ  (Read features)
‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Model      ‚îÇ
         ‚îÇ  Registry    ‚îÇ
         ‚îÇ  (MLflow)    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ (Load model)
                ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Inference   ‚îÇ
         ‚îÇ   Service    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Responsibilities

| Component | Responsibility | Technology | SLA |
|-----------|---------------|------------|-----|
| **Kafka** | Event streaming, transaction ingestion | Apache Kafka 3.x | 99.9% uptime |
| **Feature Engineering Consumer** | Compute velocity features, update Redis | Python + Faust | <50ms processing |
| **Redis Feature Store** | Real-time feature serving | Redis 7.x (Cluster) | <10ms p99 read |
| **Fraud Detection Service** | Orchestrate inference, apply business logic | FastAPI + Uvicorn | <200ms p99 |
| **Model Registry** | Version control, A/B testing | MLflow | N/A |
| **Monitoring** | Metrics, alerts, dashboards | Prometheus + Grafana | Real-time |

---

## Sprint Planning: 4-Week Implementation

### Week 1: Foundation & Streaming Layer

#### **Ticket 1.1: Kafka Cluster Setup & Topic Design**
**Story Points**: 5  
**Owner**: Data Engineer

**Acceptance Criteria:**
- [ ] Kafka cluster deployed (3 brokers, ZooKeeper/KRaft)
- [ ] Topic `transactions` created with:
  - Partitions: 12 (based on expected throughput)
  - Replication factor: 3
  - Retention: 7 days
  - Compression: `lz4`
- [ ] Schema registry configured with Avro schema
- [ ] Producer/consumer ACLs configured

**Schema Definition (Avro):**
```json
{
  "type": "record",
  "name": "Transaction",
  "fields": [
    {"name": "transaction_id", "type": "string"},
    {"name": "card_id", "type": "string"},
    {"name": "user_id", "type": "string"},
    {"name": "amount", "type": "double"},
    {"name": "merchant_id", "type": "string"},
    {"name": "merchant_category", "type": "string"},
    {"name": "timestamp", "type": "long"},
    {"name": "location_lat", "type": ["null", "double"]},
    {"name": "location_lon", "type": ["null", "double"]},
    {"name": "device_id", "type": ["null", "string"]},
    {"name": "ip_address", "type": ["null", "string"]}
  ]
}
```

**Testing:**
- Produce 10k test transactions
- Verify partition distribution
- Measure end-to-end latency (producer ‚Üí consumer)

---

#### **Ticket 1.2: Transaction Producer Simulator**
**Story Points**: 3  
**Owner**: Backend Engineer

**Acceptance Criteria:**
- [ ] Python script to generate realistic transaction data
- [ ] Configurable fraud injection rate (default 0.2%)
- [ ] Supports burst traffic patterns
- [ ] Publishes to Kafka with proper partitioning

**Implementation Notes:**
```python
# Key features to simulate
- Normal user behavior (daily patterns, favorite merchants)
- Fraud patterns (velocity attacks, geographic anomalies)
- Edge cases (international transactions, high-value purchases)
```

---

#### **Ticket 1.3: Monitoring & Alerting Setup**
**Story Points**: 3  
**Owner**: DevOps Engineer

**Acceptance Criteria:**
- [ ] Prometheus deployed with Kafka exporter
- [ ] Grafana dashboards for:
  - Kafka lag per consumer group
  - Throughput (messages/sec)
  - Error rates
- [ ] Alerts configured:
  - Consumer lag >10k messages
  - Broker down
  - Disk usage >80%

---

### Week 2: Feature Store & Real-Time Features

#### **Ticket 2.1: Redis Cluster Deployment**
**Story Points**: 5  
**Owner**: Infrastructure Engineer

**Acceptance Criteria:**
- [ ] Redis Cluster deployed (6 nodes: 3 master, 3 replica)
- [ ] Configured for low-latency:
  - `maxmemory-policy`: `allkeys-lru`
  - `save ""` (disable RDB snapshots for speed)
  - AOF enabled with `appendfsync everysec`
- [ ] Connection pooling configured (min=10, max=50)
- [ ] Monitoring via `redis_exporter`

**Capacity Planning:**
- Expected keys: ~10M active cards
- Avg key size: 2KB (features per card)
- Total memory: ~20GB + 30% overhead = **26GB**

---

#### **Ticket 2.2: Feature Engineering Consumer**
**Story Points**: 8  
**Owner**: ML Engineer

**Acceptance Criteria:**
- [ ] Faust stream processor consuming from `transactions` topic
- [ ] Compute velocity features:
  - `tx_count_last_10m`, `tx_count_last_1h`, `tx_count_last_24h`
  - `total_amount_last_10m`, `total_amount_last_1h`
  - `unique_merchants_last_24h`
  - `time_since_last_tx` (seconds)
- [ ] Update Redis with TTL-based expiration
- [ ] Handle late-arriving events (watermarking)

**Feature Schema (Redis Hash):**
```python
key = f"features:card:{card_id}"
fields = {
    "tx_count_10m": 3,
    "tx_count_1h": 12,
    "tx_count_24h": 45,
    "total_amount_10m": 450.00,
    "total_amount_1h": 1200.50,
    "unique_merchants_24h": 8,
    "last_tx_timestamp": 1675890123,
    "avg_tx_amount_30d": 85.30,  # Pre-computed batch feature
    "updated_at": 1675890125
}
```

**Performance Target:**
- Process 1000 events/sec per consumer instance
- Update Redis in <30ms p99

---

#### **Ticket 2.3: Batch Feature Pipeline (Airflow)**
**Story Points**: 5  
**Owner**: Data Engineer

**Acceptance Criteria:**
- [ ] Daily Airflow DAG to compute:
  - User historical features (avg transaction amount, favorite merchants)
  - Merchant risk scores
  - Geographic patterns
- [ ] Store in Redis with 30-day TTL
- [ ] Backfill historical data for existing users

---

### Week 3: Model Training & Inference Service

#### **Ticket 3.1: Model Training Pipeline**
**Story Points**: 8  
**Owner**: ML Engineer

**Acceptance Criteria:**
- [ ] Training dataset prepared (6 months historical data)
- [ ] Feature engineering pipeline (same as real-time)
- [ ] Model trained: **LightGBM** (chosen for speed + performance)
  - Hyperparameter tuning via Optuna
  - Target metric: PR-AUC
  - Class imbalance handling: `scale_pos_weight`
- [ ] Model registered in MLflow with:
  - Metrics (PR-AUC, precision, recall)
  - Feature importance
  - Training config

**Model Selection Rationale:**
| Model | PR-AUC | Latency (p99) | Pros | Cons |
|-------|--------|---------------|------|------|
| Logistic Regression | 0.72 | 5ms | Fast, interpretable | Poor performance |
| Random Forest | 0.81 | 45ms | Good performance | Slow inference |
| **LightGBM** | **0.86** | **25ms** | **Best balance** | Requires tuning |
| XGBoost | 0.85 | 35ms | Robust | Slower than LGBM |
| Neural Network | 0.84 | 60ms | Flexible | Slow, hard to debug |

---

#### **Ticket 3.2: FastAPI Inference Service**
**Story Points**: 8  
**Owner**: Backend Engineer

**Acceptance Criteria:**
- [ ] FastAPI service with endpoints:
  - `POST /predict` - Single transaction scoring
  - `POST /predict/batch` - Batch scoring
  - `GET /health` - Health check
  - `GET /metrics` - Prometheus metrics
- [ ] Model loaded from MLflow on startup
- [ ] Feature retrieval from Redis
- [ ] Request validation (Pydantic models)
- [ ] Error handling & fallback logic

**API Contract:**
```python
# Request
{
  "transaction_id": "tx_123456",
  "card_id": "card_789",
  "amount": 1250.00,
  "merchant_id": "merch_456",
  "merchant_category": "ELECTRONICS",
  "timestamp": 1675890123,
  "location": {"lat": 37.7749, "lon": -122.4194}
}

# Response
{
  "transaction_id": "tx_123456",
  "fraud_probability": 0.87,
  "risk_level": "HIGH",  # LOW/MEDIUM/HIGH
  "decision": "BLOCK",   # APPROVE/REVIEW/BLOCK
  "latency_ms": 145,
  "model_version": "v1.2.3",
  "features_used": 24
}
```

**Latency Optimization:**
- Async Redis calls (`aioredis`)
- Model loaded in memory (no disk I/O)
- Connection pooling
- Response caching for duplicate requests (1-second TTL)

---

#### **Ticket 3.3: Containerization & Deployment**
**Story Points**: 5  
**Owner**: DevOps Engineer

**Acceptance Criteria:**
- [ ] Dockerfile with multi-stage build:
  - Stage 1: Build dependencies
  - Stage 2: Runtime (slim Python image)
- [ ] Docker Compose for local testing
- [ ] Kubernetes manifests:
  - Deployment (3 replicas, rolling update)
  - Service (ClusterIP)
  - HPA (scale 3-10 pods based on CPU)
  - ConfigMap for environment variables
- [ ] Health checks configured (liveness + readiness)

**Dockerfile Snippet:**
```dockerfile
FROM python:3.11-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

---

### Week 4: Integration, Testing & Observability

#### **Ticket 4.1: End-to-End Integration**
**Story Points**: 5  
**Owner**: Full Team

**Acceptance Criteria:**
- [ ] Full pipeline tested: Kafka ‚Üí Feature Store ‚Üí Inference
- [ ] Load testing with 1000 req/sec
- [ ] Latency p99 <200ms verified
- [ ] Error rate <0.1%

---

#### **Ticket 4.2: Model Performance Monitoring**
**Story Points**: 5  
**Owner**: ML Engineer

**Acceptance Criteria:**
- [ ] Log predictions to database (transaction_id, prediction, actual_label)
- [ ] Daily batch job to compute:
  - Precision, Recall, PR-AUC on labeled data
  - Feature drift (PSI for top 10 features)
- [ ] Alerts:
  - PR-AUC drops >5% from baseline
  - Feature drift PSI >0.25

**Data Drift Detection (PSI):**
```python
def calculate_psi(expected, actual, bins=10):
    """Population Stability Index"""
    expected_percents = np.histogram(expected, bins)[0] / len(expected)
    actual_percents = np.histogram(actual, bins)[0] / len(actual)
    psi = np.sum((actual_percents - expected_percents) * 
                 np.log(actual_percents / expected_percents))
    return psi
```

---

#### **Ticket 4.3: Explainability & Audit Logs**
**Story Points**: 3  
**Owner**: ML Engineer

**Acceptance Criteria:**
- [ ] SHAP values computed for flagged transactions
- [ ] Audit logs stored:
  - Transaction details
  - Model prediction + confidence
  - Top 5 contributing features
  - Decision rationale
- [ ] API endpoint: `GET /explain/{transaction_id}`

---

## Senior Review Edge Cases

### Question 1: **Fail-Open vs. Fail-Closed Strategy**

**Question:**  
*"What happens if the model service times out or Redis is unavailable? Do you fail-open (approve transaction) or fail-closed (block transaction)? How do you decide?"*

**Answer:**

**Strategy: Tiered Fallback with Risk-Based Decision**

```python
def make_decision(transaction, timeout_occurred=False, redis_unavailable=False):
    if timeout_occurred or redis_unavailable:
        # Fallback to simplified rule-based system
        risk_score = evaluate_simple_rules(transaction)
        
        if transaction.amount > 10000:
            # High-value: Fail-closed (safer)
            return "BLOCK", "FALLBACK_HIGH_VALUE"
        elif risk_score > 0.7:
            # Medium risk: Send to manual review
            return "REVIEW", "FALLBACK_RULE_BASED"
        else:
            # Low risk: Fail-open (better UX)
            return "APPROVE", "FALLBACK_LOW_RISK"
    
    # Normal ML-based decision
    return ml_predict(transaction)
```

**Rationale:**
- **Fail-open for low-risk**: Prevents customer friction on legitimate transactions
- **Fail-closed for high-value**: Limits financial exposure
- **Hybrid approach**: Uses simplified rules as backup (e.g., amount thresholds, merchant category blacklists)
- **Monitoring**: Track fallback rate; alert if >5% of traffic uses fallback

**Business Alignment:**
- Discussed with Risk team: Acceptable fraud loss during outages = $50k/hour
- Fail-open on <$500 transactions (avg fraud loss: $200/tx)
- Fail-closed on >$5000 transactions

---

### Question 2: **Cold Start Problem**

**Question:**  
*"How do you handle fraud detection for brand new users or cards with no historical data?"*

**Answer:**

**Multi-Pronged Approach:**

1. **Default Feature Values:**
   ```python
   def get_features(card_id):
       features = redis.hgetall(f"features:card:{card_id}")
       if not features:
           # Use population-level defaults
           features = {
               "tx_count_10m": 0,
               "avg_tx_amount_30d": GLOBAL_AVG_TX_AMOUNT,  # e.g., $75
               "unique_merchants_24h": 0,
               "is_new_card": 1  # Feature flag for model
           }
       return features
   ```

2. **Model Trained on Cold Start:**
   - Include `is_new_card` as a feature
   - Model learns that new cards have different risk profiles
   - Separate calibration for new vs. existing users

3. **Conservative Thresholds:**
   - First 10 transactions: Lower approval threshold (more scrutiny)
   - Gradual relaxation as history builds

4. **External Data Enrichment:**
   - Device fingerprinting (new device = higher risk)
   - IP reputation scores
   - Email/phone verification status

5. **Adaptive Learning:**
   - After 24 hours, re-score user's risk profile
   - Update Redis features with actual behavior

---

### Question 3: **Model Staleness & Retraining**

**Question:**  
*"Fraud patterns evolve rapidly. How do you ensure your model doesn't become stale? What's your retraining strategy?"*

**Answer:**

**Continuous Learning Pipeline:**

1. **Retraining Cadence:**
   - **Weekly retraining** on rolling 6-month window
   - **Emergency retraining** triggered by:
     - PR-AUC drop >5%
     - New fraud pattern detected (manual analyst flag)
     - Major holiday/event (Black Friday, etc.)

2. **Online Performance Monitoring:**
   ```python
   # Daily metrics computation
   if current_pr_auc < baseline_pr_auc * 0.95:
       trigger_alert("Model performance degraded")
       initiate_retraining()
   ```

3. **Shadow Deployment:**
   - New model deployed in shadow mode (logs predictions, doesn't affect decisions)
   - Compare shadow vs. production for 48 hours
   - Promote if shadow model shows ‚â•2% PR-AUC improvement

4. **Feature Drift Detection:**
   - Monitor PSI for top 10 features daily
   - If PSI >0.25, investigate and potentially retrain

5. **Label Acquisition:**
   - Fraud labels confirmed within 24-72 hours (chargebacks, manual review)
   - Continuous feedback loop to training dataset

**Preventing Catastrophic Forgetting:**
- Maintain diverse training set (include old fraud patterns)
- A/B test new model on 10% of traffic before full rollout

---

### Question 4: **Feature Leakage & Temporal Consistency**

**Question:**  
*"How do you prevent feature leakage during training? For example, using future information that wouldn't be available at inference time."*

**Answer:**

**Strict Temporal Discipline:**

1. **Point-in-Time Feature Engineering:**
   ```python
   def compute_features(transaction, as_of_timestamp):
       # Only use data BEFORE as_of_timestamp
       recent_txs = get_transactions(
           card_id=transaction.card_id,
           start=as_of_timestamp - timedelta(hours=24),
           end=as_of_timestamp  # Exclusive
       )
       return {
           "tx_count_24h": len(recent_txs),
           "total_amount_24h": sum(tx.amount for tx in recent_txs)
       }
   ```

2. **Training Data Preparation:**
   - For each transaction at time `T`, features computed using data from `[T-30d, T)`
   - Label (fraud/not fraud) confirmed at `T+3d` (after chargeback window)
   - **Crucially**: Features never use information from `T` onward

3. **Validation:**
   - Time-based train/test split (not random)
   - Train on Jan-Jun, validate on Jul-Aug, test on Sep
   - Ensures model generalizes to future data

4. **Automated Checks:**
   ```python
   def validate_no_leakage(features, transaction):
       for feature_name, feature_value in features.items():
           feature_timestamp = get_feature_timestamp(feature_name)
           assert feature_timestamp < transaction.timestamp, \
               f"Leakage detected: {feature_name} uses future data"
   ```

5. **Common Leakage Pitfalls Avoided:**
   - ‚ùå Using `merchant_fraud_rate` computed on full dataset (includes future)
   - ‚úÖ Using `merchant_fraud_rate` computed on data up to transaction time
   - ‚ùå Including `time_to_chargeback` as a feature
   - ‚úÖ Only using features available at transaction time

---

### Question 5: **Handling Imbalanced Data**

**Question:**  
*"Fraud is only 0.2% of transactions. How do you train a model that doesn't just predict 'not fraud' for everything?"*

**Answer:**

**Multi-Strategy Approach:**

1. **Class Weighting:**
   ```python
   # LightGBM
   fraud_weight = len(y) / (2 * sum(y))  # ~250 for 0.2% fraud rate
   model = lgb.LGBMClassifier(scale_pos_weight=fraud_weight)
   ```

2. **Stratified Sampling:**
   - Undersample majority class (keep all fraud + random sample of legitimate)
   - Oversample minority class (SMOTE for synthetic fraud examples)
   - **Hybrid**: 1:10 fraud-to-legitimate ratio in training set

3. **Focal Loss (if using neural networks):**
   ```python
   def focal_loss(y_true, y_pred, gamma=2.0):
       # Down-weights easy examples, focuses on hard cases
       pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)
       return -((1 - pt) ** gamma) * tf.math.log(pt)
   ```

4. **Evaluation on Imbalanced Test Set:**
   - Test set maintains natural 0.2% fraud rate
   - Metrics: PR-AUC (not accuracy!)
   - Confusion matrix at multiple thresholds

5. **Threshold Calibration:**
   - Model outputs probability; threshold determines decision
   - Optimize threshold for business objective:
     ```python
     def business_cost(y_true, y_pred, threshold):
         fp_cost = 50  # Lost transaction value
         fn_cost = 200  # Fraud loss
         decisions = (y_pred > threshold).astype(int)
         fp = sum((decisions == 1) & (y_true == 0)) * fp_cost
         fn = sum((decisions == 0) & (y_true == 1)) * fn_cost
         return fp + fn
     
     optimal_threshold = minimize(business_cost, x0=0.5)
     ```

---

### Question 6: **Explainability for Compliance**

**Question:**  
*"If you block a transaction, can you explain why? How do you satisfy GDPR/FCRA requirements for 'right to explanation'?"*

**Answer:**

**Layered Explainability:**

1. **SHAP Values (Local Explanations):**
   ```python
   import shap
   
   explainer = shap.TreeExplainer(model)
   shap_values = explainer.shap_values(transaction_features)
   
   # Top 5 contributing features
   top_features = sorted(
       zip(feature_names, shap_values),
       key=lambda x: abs(x[1]),
       reverse=True
   )[:5]
   
   # Example output:
   # 1. tx_count_last_10m = 8 (+0.42 toward fraud)
   # 2. amount = $5000 (+0.31 toward fraud)
   # 3. merchant_risk_score = 0.85 (+0.28 toward fraud)
   ```

2. **Human-Readable Explanations:**
   ```python
   def generate_explanation(shap_values, features):
       reasons = []
       if features["tx_count_last_10m"] > 5:
           reasons.append("Unusually high transaction frequency")
       if features["amount"] > features["avg_tx_amount_30d"] * 3:
           reasons.append("Transaction amount significantly higher than usual")
       if features["merchant_risk_score"] > 0.7:
           reasons.append("Merchant has elevated fraud risk")
       return reasons
   ```

3. **Audit Trail:**
   - Store in database:
     - Transaction details
     - Model version
     - Prediction + confidence
     - Top 5 SHAP features
     - Human-readable explanation
   - Retention: 7 years (compliance requirement)

4. **Customer-Facing Messaging:**
   - ‚ùå "Transaction blocked by ML model (confidence: 0.87)"
   - ‚úÖ "Transaction flagged due to unusual activity pattern. Please verify via SMS."

5. **Regulatory Compliance:**
   - **GDPR Article 22**: Right to explanation for automated decisions
   - **FCRA**: Adverse action notices must include specific reasons
   - Our system provides both technical (SHAP) and plain-language explanations

---

### Question 7: **A/B Testing & Safe Rollouts**

**Question:**  
*"How do you safely deploy a new model version without risking a production incident?"*

**Answer:**

**Progressive Rollout Strategy:**

1. **Shadow Deployment (Week 1):**
   ```python
   # Production code
   prod_prediction = prod_model.predict(features)
   shadow_prediction = shadow_model.predict(features)
   
   # Log both, but only use prod_prediction for decision
   log_predictions(prod_prediction, shadow_prediction, transaction_id)
   
   # Offline analysis
   compare_performance(prod_predictions, shadow_predictions, actual_labels)
   ```

2. **Canary Deployment (Week 2):**
   - Route 5% of traffic to new model
   - Monitor metrics:
     - Latency (should be similar)
     - Error rate (should be <0.1%)
     - Business metrics (fraud catch rate, false positives)
   - Automated rollback if metrics degrade

3. **A/B Test (Week 3):**
   - 50/50 split between old and new model
   - Statistical significance testing:
     ```python
     from scipy.stats import ttest_ind
     
     old_pr_auc = compute_pr_auc(old_model_predictions)
     new_pr_auc = compute_pr_auc(new_model_predictions)
     
     t_stat, p_value = ttest_ind(old_pr_auc, new_pr_auc)
     if p_value < 0.05 and new_pr_auc > old_pr_auc:
         promote_new_model()
     ```

4. **Full Rollout (Week 4):**
   - 100% traffic to new model
   - Keep old model warm for instant rollback

5. **Rollback Mechanism:**
   ```python
   # Feature flag in config
   if config.get("model_version") == "v2":
       model = load_model("v2")
   else:
       model = load_model("v1")  # Instant rollback via config change
   ```

6. **Guardrails:**
   - Circuit breaker: If new model error rate >1%, auto-rollback
   - Manual kill switch: On-call engineer can revert via dashboard

---

## Technical Debt & Roadmap

### What's NOT in the MVP (But Should Be)

#### 1. **Online Learning / Continuous Training**
**Why It Matters:**  
Fraud patterns evolve daily. Batch retraining (weekly) has a lag.

**Future Implementation:**
- **Incremental Learning**: Update model weights daily with new labeled data
- **Technology**: Vowpal Wabbit or River (online learning libraries)
- **Challenge**: Catastrophic forgetting, concept drift
- **Timeline**: Q2 2026 (3 months post-MVP)

---

#### 2. **Multi-Model Ensemble**
**Why It Matters:**  
Single model has blind spots. Ensemble reduces variance.

**Future Implementation:**
- **Stacking**: Combine LightGBM + XGBoost + Neural Network
- **Weighted Voting**: Based on historical performance per fraud type
- **Challenge**: Latency budget (need to parallelize inference)
- **Timeline**: Q3 2026 (6 months post-MVP)

---

#### 3. **Graph-Based Features**
**Why It Matters:**  
Fraud rings (multiple cards, same device/IP) are hard to detect with transaction-level features.

**Future Implementation:**
- **Graph Database**: Neo4j to model card-merchant-device relationships
- **Features**: PageRank, community detection, shortest path to known fraud
- **Example**: "This card shares a device with 5 other cards flagged for fraud"
- **Timeline**: Q4 2026 (9 months post-MVP)

---

#### 4. **Reinforcement Learning for Threshold Optimization**
**Why It Matters:**  
Static thresholds don't adapt to changing fraud rates or business priorities.

**Future Implementation:**
- **RL Agent**: Learns optimal threshold per user segment
- **Reward Function**: `reward = fraud_caught * $200 - false_positives * $50`
- **Technology**: Contextual bandits (e.g., Vowpal Wabbit)
- **Timeline**: 2027 (research phase)

---

#### 5. **Federated Learning for Privacy**
**Why It Matters:**  
Multi-bank collaboration without sharing raw data.

**Future Implementation:**
- **Federated Model**: Train on decentralized data (each bank's transactions)
- **Privacy**: Differential privacy, secure aggregation
- **Benefit**: Detect cross-bank fraud patterns
- **Timeline**: 2027+ (requires industry partnerships)

---

#### 6. **Advanced Monitoring: Concept Drift Detection**
**Why It Matters:**  
PSI detects feature drift, but not concept drift (relationship between features and fraud changes).

**Future Implementation:**
- **Techniques**: 
  - ADWIN (Adaptive Windowing)
  - Page-Hinkley test
  - Monitor residuals (predicted vs. actual)
- **Action**: Auto-trigger retraining when drift detected
- **Timeline**: Q2 2026

---

#### 7. **Automated Feature Engineering**
**Why It Matters:**  
Manual feature engineering is time-consuming and may miss patterns.

**Future Implementation:**
- **Tools**: Featuretools, tsfresh (automated time-series features)
- **Deep Learning**: Autoencoders for representation learning
- **Challenge**: Interpretability decreases
- **Timeline**: Q3 2026

---

### Known Limitations & Mitigations

| Limitation | Impact | Mitigation (MVP) | Long-Term Solution |
|------------|--------|------------------|-------------------|
| **No graph features** | Misses fraud rings | Rule-based device fingerprinting | Neo4j integration (Q4 2026) |
| **Weekly retraining** | 7-day lag on new patterns | Manual emergency retraining | Online learning (Q2 2026) |
| **Single model** | Potential blind spots | High PR-AUC baseline (0.85) | Ensemble (Q3 2026) |
| **No cross-bank data** | Limited fraud pattern visibility | External fraud databases | Federated learning (2027) |
| **Static thresholds** | Suboptimal for all segments | Business-rule overrides | RL-based optimization (2027) |

---

### Operational Runbook

#### Incident Response

**Scenario 1: Model Latency Spike (p99 >500ms)**
1. Check Redis latency (likely culprit)
2. Scale Redis cluster or clear cache
3. If persistent, enable fallback mode (rule-based)
4. Post-incident: Review feature retrieval logic

**Scenario 2: False Positive Spike (>10% of transactions blocked)**
1. Immediate: Lower decision threshold (0.8 ‚Üí 0.9)
2. Investigate: Check for data drift (PSI)
3. If drift detected: Emergency retraining
4. Communicate with customer support team

**Scenario 3: Model Performance Degradation (PR-AUC <0.80)**
1. Verify data quality (missing features, schema changes)
2. Check for new fraud patterns (manual review sample)
3. Initiate emergency retraining with recent data
4. Consider rolling back to previous model version

---

## Appendix: References & Resources

### Key Technologies
- **Kafka**: [Confluent Documentation](https://docs.confluent.io/)
- **Redis**: [Redis University](https://university.redis.com/)
- **LightGBM**: [Official Docs](https://lightgbm.readthedocs.io/)
- **FastAPI**: [FastAPI Guide](https://fastapi.tiangolo.com/)
- **MLflow**: [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)

### Academic Papers
1. **"Deep Learning for Fraud Detection"** - IEEE 2020
2. **"Real-Time Feature Engineering for Fraud Detection"** - KDD 2019
3. **"Handling Imbalanced Data in ML"** - JMLR 2018

### Industry Benchmarks
- **Stripe Radar**: 99.9% legitimate transactions approved, <0.1% fraud rate
- **PayPal**: 0.32% fraud rate (industry-leading)
- **Our Target**: <0.15% fraud rate, <3% false positive rate

---

## Conclusion

This specification outlines a **production-grade, interview-ready** fraud detection system that demonstrates:

‚úÖ **System Design**: Kafka streaming, Redis feature store, containerized inference  
‚úÖ **ML Engineering**: Imbalanced data handling, model monitoring, explainability  
‚úÖ **MLOps**: CI/CD for models, shadow deployments, A/B testing  
‚úÖ **Business Acumen**: Latency budgets, fail-safe strategies, cost-benefit analysis  
‚úÖ **Scalability**: Designed for 10k+ transactions/sec  
‚úÖ **Compliance**: GDPR/FCRA-compliant explanations  

**This is not a toy project.** Every design decision is justified with production considerations, edge cases, and future roadmap. When asked in an interview, you can speak to:
- Trade-offs (e.g., LightGBM vs. Neural Networks)
- Failure modes (e.g., Redis outage ‚Üí fallback rules)
- Business impact (e.g., $2.4M saved annually)

**Next Steps:**
1. Implement Week 1 tickets (Kafka setup)
2. Set up local development environment
3. Create GitHub repo with this spec as README
4. Start building! üöÄ

---

**Document Version:** 1.0  
**Last Updated:** 2026-02-06  
**Maintained By:** ML Systems Team
